% !TeX root = ../main.tex
% -*- coding: utf-8 -*-


\begin{zhaiyao}

传统的强化学习由于理论上的各种假设与限制，只适用于简单的“完全信息博弈问题”，为了能够解决实际中更常见但也更复杂的“不完全信息博弈问题”，本文首先基于模拟采样的思路对传统的理论算法进行改进，解决了不完全信息下难以建模的根本问题。又基于统计学中的 Bootstrap 思想，将模拟采样与 Bootstrap 估计值相结合，改善数据的使用效率，提升模拟近似的准确性。最后再与梯度下降法和深度神经网络相结合，进一步提升算法的运算效率，提出了针对多人不完全信息博弈的“自适应 Deep Q-Learning 算法”。实验表明，“自适应 Deep Q-Learning 算法”能够通过强化学习有效探索出多人博弈技巧，还能高效快速处理较大数量级的实时数据流。

\end{zhaiyao}

\begin{guanjianci}
强化学习；不完全信息博弈；模拟采样；Bootstrap 方法；自适应算法；神经网络
\end{guanjianci}

\begin{abstract}

Traditional reinforcement learning is only applicable to simple ``complete information game problems" due to various theoretical assumptions and limitations. In order to solve the more common but more complex ``incomplete information game problem" in practice, this paper is based on simulation first. The sampling idea improves the traditional theoretical algorithm and solves the fundamental problem which is difficult to model under incomplete information. Based on the Bootstrap idea in statistics, the analog sampling is combined with the Bootstrap estimation to improve the efficiency of data usage and improve the accuracy of the analog approximation. Finally, combined with the gradient descent method and the deep neural network, the computational efficiency of the algorithm is further improved, and an ``Adaptive Deep Q-Learning Algorithm" for multi-person incomplete information game is proposed. Experiments show that the ``Adaptive Deep Q-Learning Algorithm" can effectively explore multiplayer game skills through reinforcement learning, and also indicates that the algorithm can process large-scale real-time data streams efficiently and quickly.

\end{abstract}

\begin{keywords}
Reinforcement Learning; Incomplete Information Game; Analog Sampling; Bootstrap Method; Adaptive Algorithm; Neural Network
\end{keywords}