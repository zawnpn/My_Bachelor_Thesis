% !TeX root = ../main.tex
% -*- coding: utf-8 -*-
% !TeX root = ../main.tex
% -*- coding: utf-8 -*-

\chapter{绪论}
\label{chpt:introduction}

\section{课题背景}

在传统的机器学习方法中，常见的主要有监督学习和无监督学习\cite{goodfellow2016deep}，其中，监督学习是指给定输入 $\boldsymbol{x}$ 和输出 $\boldsymbol{y}$ 的训练集，通过学习输入 $\boldsymbol{x}$ 和输出 $\boldsymbol{y}$ 之间的对应关系的算法；而无监督学习则尝试从不带标签的训练集中推断结论，找到数据之间的隐藏结构\cite{goodfellow2016deep}\cite{bishop2006pattern}\cite{2012statsmethods}。

强化学习则是机器学习方法中有别于监督学习和无监督学习的另一类算法，在给定环境下模拟各种行为和动作，接收环境传递的激励和惩罚反馈，自行学习如何行动才能使长期收益最大化。强化学习方法与其他机器学习方法最大的区别，在于强化学习重点关注评价性反馈，而非指导性反馈。其中，评价性反馈是对样本的一个客观评分，而指导性反馈则是明确根据问题背景提供最优解信息\cite{sutton2018reinforcement}。

在传统的强化学习问题中，环境信息会明确给出，如围棋这种双人博弈游戏，由于棋盘盘面有限，且规则简单清晰，对手的当前状态和下一步状态之间的转移概率分布可以得到准确的表示\cite{silver2017mastering}。通过得知这些信息，能够清晰建立准确的环境模型，进而做到通过具体的数学分析来求得最优解。

一场博弈中，如果玩家完美掌握了对手的策略、特征、回报函数等信息，称玩家掌握{\jiacu 完全信息}，并称这样的博弈为{\jiacu 完全信息博弈}。前面提到的双人棋类游戏就是完全信息博弈\cite{marinatto2000quantum}。反之，这样的信息称为{\jiacu 不完全信息}，对应的博弈场景称为{\jiacu 不完全信息博弈}或者{\jiacu 动态博弈}\cite{marinatto2000quantum}。

在不完全信息下，由于难以重建环境模型，传统的强化学习算法很难用于处理这类问题\cite{macdermed2011markov}。如在纸牌游戏中，若要建立环境模型，首先需要得到对手可能的手牌概率分布，才能在此基础上建立状态转移概率分布，而不像完全信息下只需建立状态转移概率模型。

不完全信息博弈的这一特点导致其解空间非常大\cite{sandholm2010state}，传统的强化学习算法难以克服这一问题，因此需要改进传统的强化学习算法，使得强化学习能够得到更广泛的应用。

\section{论文研究目标和内容}

本文将先介绍传统强化学习的核心思想，然后介绍传统的“基于 Bellman 迭代求解的强化学习算法”，但由于传统强化学习算法只适合处理完全信息博弈问题，因此提出了“基于 Monte Carlo 模拟的强化学习算法”，通过模拟采样来解决不完全信息问题下不能精确建立模型的问题。

进一步地，若只是简单地进行 Monte Carlo 模拟，又将会带来数据效率上的问题，因此基于统计学中的 Bootstrap 思想\cite{efron1994introduction}\cite{2014wzjstatistics}提出了 “Q-Learning 算法”。

最后，为了提升运算上的效率，将 Q-Learning 算法与梯度下降法以及深度神经网络相结合\cite{mnih2013playing}，然后针对多模型博弈问题背景做出相应优化，提出了“自适应 Deep Q-Learning 算法”。

为了验证本文所提出的“自适应 Deep Q-Learning 算法”能够有效地处理不完全信息下的强化学习问题，选取了一种民间纸牌游戏为仿真实验载体\cite{dahl2001reinforcement}，在其规则下进行仿真模拟实验，运用“自适应 Deep Q-Learning 算法”进行训练学习，来验证这一强化学习算法的学习效果。